{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "import gensim.models\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import jit# 加速套件\n",
    "import glob\n",
    "import os\n",
    "import statistics as stat\n",
    "from scipy import stats\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import xlsxwriter\n",
    "import numpy as np\n",
    "import numpy as ndarray\n",
    "from gensim.models import word2vec\n",
    "#from gensim.models import Word2vec\n",
    "from gensim import models\n",
    "import logging\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "import jieba\n",
    "from gensim.test.utils import datapath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以下是順序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1.載入檔案,這個檔案是每篇文章的內文與平均向量\\n2.載入bin檔,這個檔案是訓練好的bin檔,裡面有每個詞的向量\\n測試開始\\n3.輸入文字:def 請輸入文字(input_words)把輸入的文章斷詞成字\\n4.獲取輸入詞的平均向量:def get_article_avgvector(wordlist):把斷完詞的單字利用先前載入的bin檔依序變成向量,加總後再取平均\\n於是現在有了輸入的文章平均詞向量,接著,只要與原本的文章向量做比對,就可以找到最相近的文章\\n5.餘弦相似度:def cos_sim(vector_a, vector_b):這個函數把輸入的向量跟5000篇新聞逐一比對餘弦相似度\\n6.比對:def 餘閒相似度找文章(input_words):這個函數把5.比對的結果存起來,再找出最相近的一篇列出來\\n'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1.載入檔案,這個檔案是每篇文章的內文與平均向量,拿出平均向量那欄,存到articles_matrix裡面\n",
    "2.載入bin檔,這個檔案是訓練好的bin檔,裡面有每個詞的向量\n",
    "測試開始\n",
    "3.輸入文字:def 請輸入文字(input_words)把輸入的文章斷詞成字\n",
    "4.獲取輸入詞的平均向量:def get_article_avgvector(wordlist):把斷完詞的單字利用先前載入的bin檔依序變成向量,加總後再取平均\n",
    "於是現在有了輸入的文章平均詞向量,接著,只要與原本的文章向量做比對,就可以找到最相近的文章\n",
    "5.餘弦相似度:def cos_sim(vector_a, vector_b):這個函數把輸入的向量跟5000篇新聞逐一比對餘弦相似度\n",
    "6.比對:def 餘閒相似度找文章(input_words):這個函數把5.比對的結果存起來,再找出最相近的一篇列出來\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.載入檔案\n",
    "article = pd.read_excel(r\"C:\\Users\\Big data\\Desktop\\class\\funcardproject\\data\\文章\\新聞\\article_news_vector _final.xlsx\")\n",
    "articles_matrix =[get_article_matrix(article,i) for i in range(5000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No.</th>\n",
       "      <th>content</th>\n",
       "      <th>article_vector_matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [No., content, article_vector_matrix]\n",
       "Index: []"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article[1:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.載入bin檔\n",
    "wv_from_bin = KeyedVectors.load_word2vec_format(datapath(r'C:\\Users\\Big data\\Desktop\\class\\funcardproject\\word2vec_model\\news\\測試100win20cbow1.bin'), binary=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[]\n",
      "\n",
      "這幾個字的平均向量是:\n",
      "[[-2.93417601e-04 -2.09834951e-04  1.94471574e-03 -2.32643331e-03\n",
      "  -1.33754325e-03  3.02580651e-04 -2.31490145e-03  1.51515502e-04\n",
      "  -7.66507175e-04 -7.17068848e-04  6.45192224e-04 -3.79476587e-05\n",
      "  -7.17547839e-04 -2.46463367e-03  9.20091232e-04 -8.27822660e-04\n",
      "   5.60617518e-05  1.23343535e-03 -3.08174494e-04  5.06262993e-04\n",
      "  -4.80698363e-04  7.74024229e-04  1.50224473e-03 -3.99442273e-04\n",
      "  -5.92899800e-04  1.04173413e-03  1.55706273e-03 -1.55226979e-03\n",
      "   1.68823951e-03 -1.61971676e-03 -8.13295657e-04  2.36193417e-04\n",
      "   2.51700025e-04 -6.16693113e-04 -2.00104690e-03  1.79474315e-04\n",
      "   8.10040336e-04  6.95554307e-04 -4.42312710e-04  9.34593845e-04\n",
      "  -4.98159898e-05 -2.38524241e-04 -2.73752282e-03 -7.50134350e-04\n",
      "  -6.89064909e-04 -1.15323528e-04 -3.14842415e-04  2.01577088e-04\n",
      "   4.59512696e-04 -1.71064364e-03 -1.00944482e-03  3.32643103e-04\n",
      "  -3.38230711e-05  1.20210683e-03 -1.33941788e-03  2.87276285e-04\n",
      "  -2.57428875e-03  2.31159502e-04 -3.88749118e-04 -2.01631291e-03\n",
      "  -2.04912201e-03  1.31785101e-03  1.28067972e-03  4.57561691e-04\n",
      "   8.63674504e-04  3.31222924e-04  2.61665974e-03 -2.23772856e-03\n",
      "  -1.75086805e-03 -1.43232476e-03  1.43339275e-03 -7.19100470e-04\n",
      "  -3.18372448e-04 -1.21390889e-03 -9.29091650e-04 -9.29002126e-04\n",
      "  -1.47912872e-03  1.82244263e-03  3.55132244e-04 -6.47574998e-05\n",
      "   4.58610186e-04 -1.02867535e-03 -2.61318800e-03 -7.97901739e-05\n",
      "   8.32045625e-04  1.32587587e-03  1.08702865e-03 -1.37875194e-03\n",
      "   4.75268520e-04  7.62930838e-04  9.83565260e-05 -1.43188110e-03\n",
      "   3.28192255e-04  2.05058721e-03 -1.29139144e-03  7.72167754e-04\n",
      "  -1.12293102e-03 -1.20762992e-03  1.36249245e-03  1.54184469e-03]]\n",
      "第 1 篇新聞最相似\n",
      "文章內文為: \n",
      " \n",
      " ------------------------------------------------------------ \n",
      " 直擊雙11電商AI倉儲　下單到出貨僅10分鐘\n",
      "\n",
      "\n",
      "　　雙11狂熱訂單爆量，電商平台拼「出貨速度」，《卡優新聞網》直擊Yahoo奇摩倉儲現場。位於桃園大溪的Yahoo「AI自動化物流中心」，從入庫、揀貨到包裝、出貨，有高達8成藉由AI處理，消費者網路下單，只要10分鐘內就能裝箱出貨，比傳統人工大幅減少60%時間，以確保雙11貨品準時送達消費者手中。\n",
      "　　Yahoo奇摩為了加快「出貨速度」，與新竹物流、漢錸科技、工研院等團隊合作，投入數億元、耗費1年半，在桃園大溪建置首座百分百MIT的「AI自動化物流中心」，並於今(2019)年10月啟用。成為全台首座導入「AI高密度動態儲揀決策技術」，也是擁有目前業界最高儲量「穿梭式自動倉儲」的物流中心。\n",
      "　　為了達到最高效率，AI人工智慧從「商品入庫」就介入運作，電腦從銷售數據、季節、節慶…等因素，決定進貨商品種類與數量，經過材積丈量機測量、串接系統資料庫，再由AI運算出最佳儲位自動入庫，數萬個儲藏格都由電腦控管。\n",
      "　　當消費者下單後，商品就會透過自動運輸帶運出，透過CAPS電子揀貨挑出每筆訂單內的物品，在「三層式包裝站」完成包裝後，最後以分揀系統配送給貨運公司出車，全程只須10分鐘就能完成。\n",
      "　　Yahoo奇摩表示，在整個自動化流程中，僅「入庫」、「包裝」與「出貨」等3個階段須要使用人力，不僅全程無紙化，更有高達8成決策都由「電腦」決定，比起傳統人工揀貨減少6成出貨時間，提高10倍產能，準確率更提升3成。即使在雙11貨量爆增到平時的3倍，也都能快速完成出貨，即是Yahoo奇摩保證全台24小時到貨的關鍵。\n",
      "　　不過，針對「大材積」商品，例如整箱衛生紙、尿布、大電視…等，傳統人工揀貨反而比自動化速度快，目前也還沒有自動化冷鏈倉儲功能。Yahoo奇摩指出，已計畫導入載重力更大的無人搬運車，預期將能補足自動化倉儲在運送大材積商品的缺口，冷凍、冷藏商品則有溫度管控問題，現階段傾向由廠商直接出貨，不過相關系統已在研發中。\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#測試開始_餘閒相似度\n",
    "input_words= input()\n",
    "input_vector_matrix =請輸入文字(input_words)\n",
    "餘閒相似度找文章(input_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "槼\n",
      "['槼']\n",
      "\n",
      "這幾個字的平均向量是:\n",
      "[[ 2.08755693e-04 -5.01122675e-04 -1.29886169e-03  3.03323148e-04\n",
      "  -7.16154245e-05 -2.45773757e-04 -1.50207686e-03  3.90264577e-05\n",
      "  -2.02148291e-03 -7.96370266e-04 -1.57396111e-03  1.11444748e-03\n",
      "   8.37674772e-04  9.73139104e-05  2.16050714e-04  1.49619731e-03\n",
      "  -6.15576166e-04  5.87106973e-04 -1.55895669e-03 -3.53533942e-05\n",
      "   1.55223519e-04 -1.32236542e-04  8.17690379e-05 -5.91848395e-04\n",
      "  -9.10355477e-04  1.66113593e-03  1.46988418e-03 -4.62827767e-04\n",
      "  -1.22714904e-04 -7.88273348e-04 -2.45518837e-04 -2.71476951e-04\n",
      "  -4.41119919e-04 -1.51924498e-04 -1.17694912e-03  1.82697520e-04\n",
      "   4.43123630e-04  4.47559811e-04 -1.86704565e-03  8.91451666e-04\n",
      "   5.66984119e-04  1.29529639e-04 -1.66215905e-04  8.36389299e-05\n",
      "  -4.98605426e-04 -4.48772887e-04  2.83639907e-04 -2.99409701e-04\n",
      "  -6.31385308e-04 -6.67086744e-04 -1.35480834e-04  2.15654494e-04\n",
      "  -3.58017933e-05 -4.95099230e-04  1.18449365e-03 -5.61687455e-04\n",
      "  -2.10800237e-04  1.16956525e-03 -1.35498805e-04 -5.97576029e-04\n",
      "  -5.30131452e-04  1.03280996e-03  3.60007747e-04  3.39832011e-04\n",
      "   3.14622943e-04  8.36625404e-04  3.09880066e-04 -1.72494829e-03\n",
      "  -1.56985957e-03 -2.67863856e-04 -6.11084979e-04  4.68487269e-04\n",
      "  -1.44995982e-04 -6.07175811e-04  8.28843331e-04 -2.55522900e-04\n",
      "  -2.32311286e-04  1.14861224e-03  5.09082340e-04 -4.92742634e-04\n",
      "  -1.84187651e-04  3.53427953e-04 -1.54469081e-03 -3.65288608e-04\n",
      "   1.28105097e-03  5.89803152e-04 -4.37743292e-04 -1.37975893e-03\n",
      "  -8.80708569e-04 -4.56604612e-04  7.55040383e-04  2.31263941e-04\n",
      "  -3.80939309e-04  4.14580660e-04 -7.40982767e-04  5.82799141e-04\n",
      "   4.19995922e-05 -8.22642702e-04  3.19830724e-04 -2.19724272e-04]]\n",
      "第 858 篇新聞最相似\n",
      "文章內文為: \n",
      " \n",
      " ------------------------------------------------------------ \n",
      "  快訊／六月信用卡刷出3282億元　創台歷年新高  \n",
      "\t\t\t\t\t   \t\t金管會公布信用卡最新統計，六月單月刷卡金額達到3282億元，創下歷年單月新高，銀行局表示，主要是刷卡繳交綜所稅以及房屋稅導致。最HOT話題在這！想跟上時事，快點我加入TVBS新聞LINE好友！ \n"
     ]
    }
   ],
   "source": [
    "#測試開始_歐式距離\n",
    "#歐式距離效果較差\n",
    "input_words= input()\n",
    "input_vector_matrix =請輸入文字(input_words)\n",
    "歐式距離找文章(input_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #4.獲取輸入詞的平均向量\n",
    "# def get_article_avgvector(wordlist):\n",
    "#     #取每篇文章平均向量\n",
    "#     # x=np.matrix(wv_from_bin[word])安安?\n",
    "#     len_wordlist = len(wordlist)\n",
    "#     input_avgvector_matrix=0\n",
    "\n",
    "#     for word in wordlist:\n",
    "#         try:\n",
    "#             x=np.matrix(wv_from_bin[word])\n",
    "#             input_avgvector_matrix+=x\n",
    "#             input_avgvector_matrix = input_avgvector_matrix/len_wordlist\n",
    "#         except:\n",
    "#             pass\n",
    "#     return(input_avgvector_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.獲取輸入詞的平均向量\n",
    "def get_article_avgvector(wordlist):\n",
    "    #取每篇文章平均向量\n",
    "    # x=np.matrix(wv_from_bin[word])安安?\n",
    "    len_wordlist = len(wordlist)\n",
    "    input_avgvector_matrix=[]\n",
    "    \n",
    "    \n",
    "    for word in wordlist:\n",
    "        try:\n",
    "            x=np.matrix(wv_from_bin[word])\n",
    "            input_avgvector_matrix+=x\n",
    "            input_avgvector_matrix = input_avgvector_matrix/len_wordlist\n",
    "        except:\n",
    "            pass\n",
    "    #若沒有輸入文章或輸入的詞不在bin檔則回傳第0篇文章向量\n",
    "    if input_avgvector_matrix ==[]:\n",
    "        input_avgvector_matrix = articles_matrix[1]\n",
    "    \n",
    "     \n",
    "    return(input_avgvector_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.輸入文字\n",
    "def 請輸入文字(input_words):\n",
    "    #隨意輸入文字\n",
    "    #轉DataFrame\n",
    "    # input_words=pd.DataFrame(input_words,columns=['content'])\n",
    "    #斷詞\n",
    "    wordlist=jieba.lcut(input_words,cut_all=False)\n",
    "    # df1=斷詞(input_words)\n",
    "    #清乾淨\n",
    "    # news = 整理(cut)\n",
    "    # 吐出來\n",
    "    # output_words = input_words['seg'][0]\n",
    "    # output_words = output_words.split(' ')\n",
    "    print(wordlist)\n",
    "    input_vector_matrix = get_article_avgvector(wordlist)\n",
    "    print()\n",
    "    print(\"這幾個字的平均向量是:\")\n",
    "    print(input_vector_matrix)\n",
    "    return(input_vector_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.餘弦相似度\n",
    "def cos_sim(vector_a, vector_b):\n",
    "    \"\"\"\n",
    "    计算两个向量之间的余弦相似度\n",
    "    :param vector_a: 向量 a \n",
    "    :param vector_b: 向量 b\n",
    "    :return: sim\n",
    "    \"\"\"\n",
    "    vector_a = np.mat(vector_a)\n",
    "    vector_b = np.mat(vector_b)\n",
    "    num = float(vector_a * vector_b.T)\n",
    "    denom = np.linalg.norm(vector_a) * np.linalg.norm(vector_b)\n",
    "    cos = num / denom\n",
    "    sim = 0.5 + 0.5 * cos\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#撈出存在excel的向量轉為matrix\n",
    "def get_article_matrix(article,i):\n",
    "    aa = article.loc[:,[\"article_vector_matrix\"]][i:i+1]\n",
    "    #轉ARRAY再轉list\n",
    "    b = np.array(aa)\n",
    "    b=b[0].tolist()#list\n",
    "    # 切\n",
    "    c = str(b[0]).split(',')\n",
    "    article_matrix = np.mat(c).astype(float)\n",
    "    return(article_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#比對\n",
    "def 餘閒相似度找文章(input_words):\n",
    "    articles_matrix_list=[]\n",
    "    for b in range(5000):\n",
    "        結果=cos_sim(input_vector_matrix,articles_matrix[b])\n",
    "        articles_matrix_list.append(結果)\n",
    "    print(\"第\",articles_matrix_list.index(max(articles_matrix_list)),\"篇新聞最相似\")\n",
    "    哭喔 = articles_matrix_list.index(max(articles_matrix_list))\n",
    "    print(\"文章內文為:\",\"\\n\",\"\\n\",\"------------------------------------------------------------\",\"\\n\",np.array(article[哭喔:哭喔+1]['content'])[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def 歐式距離找文章(input_words):\n",
    "    articles_matrix_list=[]\n",
    "    for b in range(5000):\n",
    "        結果=np.linalg.norm(input_vector_matrix - articles_matrix[b])\n",
    "        articles_matrix_list.append(結果)\n",
    "    \n",
    "    print(\"第\",articles_matrix_list.index(max(articles_matrix_list)),\"篇新聞最相似\")\n",
    "    哭喔 = articles_matrix_list.index(max(articles_matrix_list))\n",
    "    print(\"文章內文為:\",\"\\n\",\"\\n\",\"------------------------------------------------------------\",\"\\n\",np.array(article[哭喔:哭喔+1]['content'])[0])\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 斷詞套件\n",
    "def 斷詞(news):\n",
    "    df1=news\n",
    "    df1['seg']=None\n",
    "    df1['word_freq']=None\n",
    "    n=len(news)\n",
    "    alltext=[]\n",
    "    stopset=set()\n",
    "    stop2=['/n','']\n",
    "    with open('C:\\\\Users\\\\Big data\\\\Desktop\\\\字詞貼標\\\\DB104-master\\\\stop.txt','r',encoding='ISO-8859-1') as s:\n",
    "        for line in s:\n",
    "            stopset.add(line.strip('\\n'))\n",
    "    jieba.load_userdict('C:\\\\Users\\\\Big data\\\\Desktop\\\\字詞貼標\\\\DB104-master\\\\userdict.txt')\n",
    "    for i in range(n):\n",
    "        seg=''\n",
    "        wf={}\n",
    "        text= str(df1.loc[i]['content'])\n",
    "        cut=jieba.cut(text,cut_all=False)\n",
    "        for j in cut:\n",
    "            if j not in stopset:\n",
    "                seg +=j+' '\n",
    "                seg = seg.replace(\"\\n\",'')\n",
    "        df1['seg'][i]=seg\n",
    "        for w in seg.split(' '):\n",
    "            if w not in wf:\n",
    "                wf[w]=1\n",
    "            else:\n",
    "                wf[w]+=1\n",
    "        word_list=[(k,wf[k]) for k in wf if k not in stop2 ]\n",
    "        word_list.sort(key=lambda a :a[1],reverse=True)\n",
    "        df1['word_freq'][i]=word_list\n",
    "    return(df1)\n",
    "\n",
    "\n",
    "\n",
    "# 斷詞整理(去除標點符號)\n",
    "def 整理(df1):\n",
    "    datanews = df1[\"seg\"]\n",
    "    # 轉list\n",
    "    train_data = np.array(datanews)#np.ndarray()\n",
    "    datanews=train_data.tolist()#list\n",
    "    df1=''\n",
    "    for datanew in datanews:\n",
    "        df1+=str(datanew)+'\\n'\n",
    "    df1=df1.replace('[','').replace('\\'','').replace('「 ','').replace('」 ','').replace('」 ','').replace('、 ','').replace('： ','')\n",
    "    df1=df1.replace('《 ','').replace('》 ','').replace('） ','').replace('( ','').replace('／ ','').replace('， ','')\n",
    "    df1=df1.replace('╱ ','').replace('！ ','').replace('？ ','').replace('（ ','').replace('。 ','').replace('； ','').replace('… ','')\n",
    "    df1=df1.replace(']','').replace(' :','').replace('\\u3000','').replace('\\n','')\n",
    "    df2=df1\n",
    "    for i in range(len(df2)):\n",
    "        df1=' '.join(df2.split())\n",
    "    return(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
