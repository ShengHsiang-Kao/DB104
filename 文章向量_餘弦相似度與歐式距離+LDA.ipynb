{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "import gensim.models\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import jit# 加速套件\n",
    "import glob\n",
    "import os\n",
    "import statistics as stat\n",
    "from scipy import stats\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import xlsxwriter\n",
    "import numpy as np\n",
    "import numpy as ndarray\n",
    "from gensim.models import word2vec\n",
    "#from gensim.models import Word2vec\n",
    "from gensim import models\n",
    "import logging\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "import jieba\n",
    "from gensim.test.utils import datapath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以下是順序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1.載入檔案,這個檔案是每篇文章的內文與平均向量,拿出平均向量那欄,存到articles_matrix裡面\\n2.載入bin檔,這個檔案是訓練好的bin檔,裡面有每個詞的向量\\n測試開始\\n3.輸入文字:def 請輸入文字(input_words)把輸入的文章斷詞成字\\n4.獲取輸入詞的平均向量:def get_article_avgvector(wordlist):把斷完詞的單字利用先前載入的bin檔依序變成向量,加總後再取平均\\n於是現在有了輸入的文章平均詞向量,接著,只要與原本的文章向量做比對,就可以找到最相近的文章\\n5.餘弦相似度:def cos_sim(vector_a, vector_b):這個函數把輸入的向量跟5000篇新聞逐一比對餘弦相似度\\n6.比對:def 餘閒相似度找文章(input_words):這個函數把5.比對的結果存起來,再找出最相近的一篇列出來\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1.載入檔案,這個檔案是每篇文章的內文與平均向量,拿出平均向量那欄,存到articles_matrix裡面\n",
    "2.載入bin檔,這個檔案是訓練好的bin檔,裡面有每個詞的向量\n",
    "測試開始\n",
    "3.輸入文字:def 請輸入文字(input_words)把輸入的文章斷詞成字\n",
    "4.獲取輸入詞的平均向量:def get_article_avgvector(wordlist):把斷完詞的單字利用先前載入的bin檔依序變成向量,加總後再取平均\n",
    "於是現在有了輸入的文章平均詞向量,接著,只要與原本的文章向量做比對,就可以找到最相近的文章\n",
    "5.餘弦相似度:def cos_sim(vector_a, vector_b):這個函數把輸入的向量跟5000篇新聞逐一比對餘弦相似度\n",
    "6.比對:def 餘閒相似度找文章(input_words):這個函數把5.比對的結果存起來,再找出最相近的一篇列出來\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5594"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1.載入文章檔案\n",
    "articles = pd.read_excel(r\"C:\\Users\\Big data\\Desktop\\class\\funcardproject\\LDA\\LDA_向量_20群.xlsx\")\n",
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>word</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['我們', '你', '我', '被', '就是', '這個', '民眾', '可以', ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['大陸', '我', '你', '就是', '日本', '現在', '可以', '因為',...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>['新聞', '活動', '您', '金幣', '該', '好', '按', '優質', '...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>['贊助', '與', '台灣', '心中', '中獎', '服務', '金融', '網點'...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>['日', '餐廳', '飯店', '美食', '台北', '吃', '送', '及', '...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>['旅遊', '台灣', '日', '旅客', '日本', '台北', '機場', '元',...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>['銀行', '金管會', '資料', '表示', '客戶', '業務', '交易', '或...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>['回饋', '消費', '優惠', '與', '元', '現金回饋', '最高', '卡'...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>['繳稅', '蘋果', 'IPHONE', '繳納', '報稅', '申報', '遊戲',...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>['他', '總統', '我', '他們', '被', '年', '立委', '人', '美...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>['億元', '今年', '經濟日報', '為', '成長', '金額', '年', '及'...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>['香港', '廣告', '年', '為', '代言', '運動', '粉絲', '英國',...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>['盜刷', '警方', '嫌犯', '被', '被害人', '詐騙', '集團', '詐騙...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>['繳費', '車主', '黃', '罰單', '收費', '萬元', '車輛', '機車'...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>['他', '她', '被', '我', '發現', '後', '男子', '一名', '陳...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>['銀行', '刷卡', '民眾', '消費', '優惠', '卡', '元', '可以',...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>['手機', '支付', '使用', '行動支付', 'APP', '服務', '民眾', ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>['元', '發票', '消費者', '違約金', '消基會', '油價', '業者', '...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>['加油', '加油站', '計程車', '司機', '乘客', '自助', '駕駛', '...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>['美國', '美元', '市場', '經濟', '全球', '投資', '年', '億',...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  label                                               word  freq\n",
       "0            0      0  ['我們', '你', '我', '被', '就是', '這個', '民眾', '可以', ...     0\n",
       "1            1      1  ['大陸', '我', '你', '就是', '日本', '現在', '可以', '因為',...     0\n",
       "2            2      2  ['新聞', '活動', '您', '金幣', '該', '好', '按', '優質', '...     0\n",
       "3            3      3  ['贊助', '與', '台灣', '心中', '中獎', '服務', '金融', '網點'...     0\n",
       "4            4      4  ['日', '餐廳', '飯店', '美食', '台北', '吃', '送', '及', '...     0\n",
       "5            5      5  ['旅遊', '台灣', '日', '旅客', '日本', '台北', '機場', '元',...     0\n",
       "6            6      6  ['銀行', '金管會', '資料', '表示', '客戶', '業務', '交易', '或...     0\n",
       "7            7      7  ['回饋', '消費', '優惠', '與', '元', '現金回饋', '最高', '卡'...     0\n",
       "8            8      8  ['繳稅', '蘋果', 'IPHONE', '繳納', '報稅', '申報', '遊戲',...     0\n",
       "9            9      9  ['他', '總統', '我', '他們', '被', '年', '立委', '人', '美...     0\n",
       "10          10     10  ['億元', '今年', '經濟日報', '為', '成長', '金額', '年', '及'...     0\n",
       "11          11     11  ['香港', '廣告', '年', '為', '代言', '運動', '粉絲', '英國',...     0\n",
       "12          12     12  ['盜刷', '警方', '嫌犯', '被', '被害人', '詐騙', '集團', '詐騙...     0\n",
       "13          13     13  ['繳費', '車主', '黃', '罰單', '收費', '萬元', '車輛', '機車'...     0\n",
       "14          14     14  ['他', '她', '被', '我', '發現', '後', '男子', '一名', '陳...     0\n",
       "15          15     15  ['銀行', '刷卡', '民眾', '消費', '優惠', '卡', '元', '可以',...     0\n",
       "16          16     16  ['手機', '支付', '使用', '行動支付', 'APP', '服務', '民眾', ...     0\n",
       "17          17     17  ['元', '發票', '消費者', '違約金', '消基會', '油價', '業者', '...     0\n",
       "18          18     18  ['加油', '加油站', '計程車', '司機', '乘客', '自助', '駕駛', '...     0\n",
       "19          19     19  ['美國', '美元', '市場', '經濟', '全球', '投資', '年', '億',...     0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1.載入LDA分群檔案\n",
    "group_lists = pd.read_excel(r\"C:\\Users\\Big data\\Desktop\\class\\funcardproject\\LDA\\LDA_20群_100詞.xlsx\")\n",
    "group_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.載入bin檔\n",
    "wv_from_bin = KeyedVectors.load_word2vec_format(datapath(r'C:\\Users\\Big data\\Desktop\\class\\funcardproject\\word2vec_model\\news\\測試100win20cbow1.bin'), binary=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 測試區"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "滙豐銀推升級現金回饋御璽卡 哩程數1元換2哩被號稱是刷卡神卡的中國信託LINE Pay卡，今年度回饋縮減後，各家信用卡後來居上爭搶刷卡神卡地位。外商銀行滙豐銀行力推自家現金回饋御璽卡，2019年宣布全面升級，除了原先不設門檻國內消費1.22％、海外消費2.22％現金回饋外，更首次開放卡友可以將回饋金累計兌換飛行里程數。此次全面升級回饋金累\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "DataFrame constructor not properly called!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-140-31c865e6b345>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0minput_words\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0minput_vector_matrix\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0m請輸入文字\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mget_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mgroup_articles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup_article\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0marticles_matrix\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mget_article_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup_articles\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup_articles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-139-7afa6e55bb58>\u001b[0m in \u001b[0;36m請輸入文字\u001b[1;34m(input_words)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m#隨意輸入文字\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m#轉DataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0minput_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'content'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;31m#斷詞\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#     wordlist=jieba.lcut(input_words,cut_all=False)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    466\u001b[0m                                    dtype=values.dtype, copy=False)\n\u001b[0;32m    467\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'DataFrame constructor not properly called!'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmgr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: DataFrame constructor not properly called!"
     ]
    }
   ],
   "source": [
    "input_words= input()\n",
    "input_vector_matrix =請輸入文字(input_words)\n",
    "label= get_label(input_words)\n",
    "group_articles = group_article(label)\n",
    "articles_matrix =[get_article_matrix(group_articles,i) for i in range(len(group_articles))]\n",
    "餘閒相似度找文章(input_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "玉山\n",
      "['玉山']\n",
      "\n",
      "這幾個字的平均向量是:\n",
      "[[ 0.19610208 -0.563665   -0.3372065  -0.5663267   0.10419251 -0.03174496\n",
      "  -0.57335347 -0.01868376 -0.77252775 -0.3306109   0.44495112  0.50069845\n",
      "  -0.19555637  0.08804778 -0.7298876  -0.24601893 -0.04557158 -0.38264757\n",
      "  -0.18670572 -0.48187992  0.4048067  -0.17634086  0.07316504 -0.4861287\n",
      "  -0.5141417   0.6257923   0.61300194  0.27603537 -0.15095751  0.04430315\n",
      "   0.12776838 -0.11363579 -0.1937178  -0.04849094 -1.1038033  -0.16659811\n",
      "   0.3701372   0.95391536 -0.35162738 -0.6978629   0.1354421   0.17245938\n",
      "  -0.27713624 -0.29105332  0.42139494 -0.44791517 -0.43316543 -0.51872057\n",
      "  -0.32615614  0.17171097  0.01827458 -0.25880343  0.50627005 -0.06921496\n",
      "   0.5086911  -0.49112234 -0.09658755  0.01427571 -0.7565906  -0.39970285\n",
      "   0.02148487 -0.17707984 -0.90400654  0.42158422 -0.02072658  0.9841369\n",
      "   0.6100574  -0.5122336  -0.73656815 -0.5097141  -0.16665676  0.1635552\n",
      "  -0.22809388  0.25000995 -0.2141154  -0.8958415  -0.07857244  0.4800984\n",
      "   0.4884535  -0.4106217  -0.3088947   0.03121278 -1.3214395   0.6111621\n",
      "   0.4182697   0.11762522  0.15772997  0.08272251 -0.3095944  -0.60688895\n",
      "   0.23399884 -0.37150428  0.3042093   0.4811323  -0.08331485  0.60731685\n",
      "  -0.17062674 -0.02142499 -0.67650694  0.10243874]]\n",
      "第 171 篇新聞最相似\n",
      "文章內文為: \n",
      " \n",
      " ------------------------------------------------------------ \n",
      " ISICDEBIT卡學生專屬留遊學現金回饋到海外留學遊學的青年學子將有更便利更優惠的金融服務永豐銀行今日發行永豐國際學生證DEBIT卡只要是學生就能申請享有國際學生證全球超過萬項優惠海外消費現金回饋高達首年發卡目標為萬張根據萬事達卡調查台灣父母送子女出國與日俱增比例從年的上升至年的為滿足海外學子金融需求永豐銀行與康文文教基金會萬事達卡合作推出國際學生證DEBIT卡學生至康文辦事處或網站取得國際學生證ISIC資格就能到忠孝新竹興大東台南高雄等永豐分行開戶辦理國際學生證DEBIT卡享有國際學生證全球個國家超過萬項優惠永豐國際學生證DEBIT卡不僅是學生證與提款卡於國內消費享現金回饋以及每月次免費跨行提款轉帳在海外消費的現金回饋更高達每月還補貼國外提款手續費最高元新台幣以單次手續費約元計算相當於每月有次提款免手續費從事國外旅遊購買飛機火車或巴士車票還有萬元新台幣旅遊平安險保障異國求學的學子永豐銀行表示以美國為例持國際學生證能以折購買紐約市通行證CITYPASS奧蘭多環球影城門票折大都會博物館與MOMA紐約現代美術館只要美元美元在其他國家如丹麥購買APPLE商品享折價澳洲更有麥當勞與肯德基指定餐點買送台灣每年發證數高達萬張永豐結合金融卡功能加入現金回饋好康預期首年就能發出萬張即日起申辦卡片前名能省下國際學生證申辦費元新台幣發卡次月底前一般消費累積滿元新台幣加贈元刷卡金開通悠遊卡功能並自動加值成功再送元刷卡金首月加總能拿到元優惠永豐銀行近個月動作不斷陸續發行三井OUTLET台中港聯名卡SPORT運動卡及國際學生證DEBIT卡具強烈企圖心截至月底為止永豐銀行流通卡數逼近萬張有效卡數超過萬張卡量名列第名與排名第的聯邦銀行萬流通卡萬有效卡差距不大永豐銀行指出在新發卡片的加持下明年可望超車聯邦銀行\n"
     ]
    }
   ],
   "source": [
    "#測試開始_餘閒相似度\n",
    "input_words= input()\n",
    "input_vector_matrix =請輸入文字(input_words)\n",
    "餘閒相似度找文章(input_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 根據分群選項輛"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_article(label):\n",
    "    group_articles=pd.DataFrame(columns=['Unnamed: 0','No.','content','article_vector_matrix','label'])\n",
    "    cc=0\n",
    "    for n,i in enumerate(articles['label']):\n",
    "        if i == label:\n",
    "            group_articles.loc[cc]=list(articles.loc[n])\n",
    "            cc+=1\n",
    "    return group_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(input_words):\n",
    "    group_lists[\"freq\"]=0\n",
    "    for n,group in enumerate(group_lists[\"word\"]):\n",
    "        for input_word in input_words:\n",
    "            if input_word in group:\n",
    "                group_lists[\"freq\"][n]+=1\n",
    "    label = np.where(group_lists[\"freq\"]==np.max(group_lists[\"freq\"]))\n",
    "    label=label[0][0]\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.獲取輸入詞的平均向量\n",
    "def get_article_avgvector(wordlist):\n",
    "    #取每篇文章平均向量\n",
    "    # x=np.matrix(wv_from_bin[word])安安?\n",
    "    len_wordlist=0\n",
    "    input_avgvector_matrix=0\n",
    "    for word in wordlist:\n",
    "\n",
    "        try:\n",
    "            x=np.matrix(wv_from_bin[word])\n",
    "            input_avgvector_matrix+=x\n",
    "            len_wordlist+=1\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "    if type(input_avgvector_matrix)==int:\n",
    "        input_avgvector_matrix = articles_matrix[1]\n",
    "    else:\n",
    "        input_avgvector_matrix = input_avgvector_matrix/len_wordlist\n",
    "    \n",
    " \n",
    "      \n",
    "    #若沒有輸入文章或輸入的詞不在bin檔則回傳第0篇文章向量\n",
    "\n",
    "    \n",
    "     \n",
    "    return(input_avgvector_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.49321005, -0.97627735, -1.4002988 , -0.68763477, -0.1221542 ,\n",
       "         -0.5081506 , -1.8594931 , -0.61623746,  0.24533285, -0.50148046,\n",
       "         -0.57171524,  0.11776888, -0.07833362, -1.0480946 , -0.02709624,\n",
       "         -0.6206844 , -0.09482106,  0.6255164 , -0.9778987 , -0.76802504,\n",
       "         -0.08358138, -0.09517649,  1.5977589 ,  0.35336128, -0.48482433,\n",
       "          0.0540897 ,  0.91026616, -0.3784378 ,  1.6129799 , -0.3451628 ,\n",
       "         -1.5321958 ,  0.13388553,  0.05983865, -1.3810012 , -0.81214154,\n",
       "          0.19814089, -0.17143549,  0.8975501 , -0.08294594, -0.7686179 ,\n",
       "         -0.84542733,  0.02836706, -0.22351918, -0.0919289 ,  0.6135563 ,\n",
       "          1.719913  , -1.4062992 ,  0.61917096,  0.41225258,  0.3907897 ,\n",
       "          0.16261263,  0.84341156, -0.21983007, -0.19169547, -0.39086354,\n",
       "         -1.2234123 , -0.7592441 ,  0.10992469, -0.70064735, -0.8791137 ,\n",
       "         -1.1320727 , -0.4950929 ,  0.19753097, -0.47997445,  0.27035797,\n",
       "          0.5783057 ,  0.5555821 ,  0.3693077 ,  0.2578745 , -1.0639296 ,\n",
       "          0.15000439, -0.47604662, -0.14871505,  0.33519515,  0.5018694 ,\n",
       "         -0.5255309 ,  0.34885353,  0.7911588 , -0.7902082 , -0.00725085,\n",
       "         -0.1160955 , -0.7060676 , -0.8587929 ,  0.15730458,  0.30203998,\n",
       "         -0.09920797, -0.25961873,  0.362034  , -0.597182  , -0.8793932 ,\n",
       "          0.4543312 , -0.70007396, -0.2694512 ,  0.03651327, -1.9549129 ,\n",
       "         -0.06064856, -0.03636876, -0.4118322 , -0.3418204 , -0.32556272]],\n",
       "       dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=np.matrix(wv_from_bin[\"旅遊\"])\n",
    "y=np.matrix(wv_from_bin[\"加油\"])\n",
    "input_avgvector_matrix=0\n",
    "input_avgvector_matrix+=x\n",
    "input_avgvector_matrix+=y\n",
    "input_avgvector_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.輸入文字\n",
    "def 請輸入文字(input_words):\n",
    "    #隨意輸入文字\n",
    "    #轉DataFrame\n",
    "    # input_words=pd.DataFrame(input_words,columns=['content'])\n",
    "    #斷詞\n",
    "    wordlist=jieba.lcut(input_words,cut_all=False)\n",
    "    # df1=斷詞(input_words)\n",
    "    #清乾淨\n",
    "    # news = 整理(cut)\n",
    "    # 吐出來\n",
    "    # output_words = input_words['seg'][0]\n",
    "    # output_words = output_words.split(' ')\n",
    "    print(wordlist)\n",
    "    input_vector_matrix = get_article_avgvector(wordlist)\n",
    "    print()\n",
    "    print(\"這幾個字的平均向量是:\")\n",
    "    print(input_vector_matrix)\n",
    "    return(input_vector_matrix)\n",
    "    return(wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.輸入文字\n",
    "def 請輸入文字(input_words):\n",
    "    #隨意輸入文字\n",
    "    #轉DataFrame\n",
    "    input_words=pd.DataFrame(input_words,columns=['content'])\n",
    "    #斷詞\n",
    "#     wordlist=jieba.lcut(input_words,cut_all=False)\n",
    "    df1=斷詞(input_words)\n",
    "    #清乾淨\n",
    "    input_words = 整理(cut)\n",
    "    # 吐出來\n",
    "    output_words = input_words['seg'][0]\n",
    "    output_words = output_words.split(' ')\n",
    "    print(output_words)\n",
    "    input_vector_matrix = get_article_avgvector(output_words)\n",
    "    print()\n",
    "    print(\"這幾個字的平均向量是:\")\n",
    "    print(input_vector_matrix)\n",
    "    return(input_vector_matrix)\n",
    "    return(wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.餘弦相似度\n",
    "def cos_sim(vector_a, vector_b):\n",
    "    \"\"\"\n",
    "    计算两个向量之间的余弦相似度\n",
    "    :param vector_a: 向量 a \n",
    "    :param vector_b: 向量 b\n",
    "    :return: sim\n",
    "    \"\"\"\n",
    "    vector_a = np.mat(vector_a)\n",
    "    vector_b = np.mat(vector_b)\n",
    "    num = float(vector_a * vector_b.T)\n",
    "    denom = np.linalg.norm(vector_a) * np.linalg.norm(vector_b)\n",
    "    cos = num / denom\n",
    "    sim = 0.5 + 0.5 * cos\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#撈出存在excel的向量轉為matrix\n",
    "def get_article_matrix(article,i):\n",
    "    aa = article.loc[:,[\"article_vector_matrix\"]][i:i+1]\n",
    "    #轉ARRAY再轉list\n",
    "    b = np.array(aa)\n",
    "    b=b[0].tolist()#list\n",
    "    # 切\n",
    "    c = str(b[0]).split(',')\n",
    "    article_matrix = np.mat(c).astype(float)\n",
    "    return(article_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 18 篇新聞最相似\n",
      "文章內文為: \n",
      " \n",
      " ------------------------------------------------------------ \n",
      " 〈獨家〉自助加油試身「手」　每公升2元省很大\n",
      "受下週油價恐怕又要漲的預期心理影響，不少人已經開始想要怎麼省油錢，但除了提早加油外，其實自助加油還是最省的選擇，但加油站業者透露，自助加油比例並沒有想像中的多，就算油價週週漲，還是只占4成，形成民眾為了搶加油大排長龍，但自助加油卻門可羅雀的景象。加油站人員：「歡迎光臨，請往前哦。」\r",
      "車子一輛接一輛，油價又要漲，精打細算民眾提早加油，但相對於得排隊等人工加油，能省錢的自助加油區卻反而沒有人。\r",
      "民眾：「自助加油麻煩，因為我不會加，所以人工加油比較方便，人工加油我就在車上，還可以聽聽音樂之類，自助加油還得下去，有點麻煩。」\r",
      "就怕自己加油浪費時間，按發油量計算，有6成民眾寧願花時間排隊，也不想自己加油。TVBS記者吳亭儀：「自助加油到底要花多少時間，我們實際找民眾來實測。」\r",
      "自助加油加1000元，拿到收據是1分51秒，加上車時間大約2分鐘，而人工加1300元的油也大約要2.5分鐘，其實時間差不多。\r",
      "民眾：「在我們可以想的這種方式，節約節省的方式，我們試著來節省，自助加油聯名卡降2元哦。」\r",
      "自助加油降幅相對大了些，但受不同信用卡影響，折扣也有高低差別，至於人工加油，50公升油箱汽車用信用卡5%現金回饋，搭配指定加油站，每公升降1.7元優惠，也能省下破百元，但降幅還是沒有自助加油來的低，開車族想省荷包，還是得先習慣自己加油。 \n"
     ]
    }
   ],
   "source": [
    "articles_matrix_list=[]\n",
    "for b in range(len(group_articles)):\n",
    "    結果=cos_sim(input_vector_matrix,articles_matrix[b])\n",
    "    articles_matrix_list.append(結果)\n",
    "#     print(\"第\",articles_matrix_list.index(max(articles_matrix_list)),\"篇新聞最相似\")\n",
    "print(\"第\",articles_matrix_list.index(max(articles_matrix_list)),\"篇新聞最相似\")\n",
    "哭喔 = articles_matrix_list.index(max(articles_matrix_list))\n",
    "print(\"文章內文為:\",\"\\n\",\"\\n\",\"------------------------------------------------------------\",\"\\n\",np.array(group_articles[哭喔:哭喔+1]['content'])[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#比對\n",
    "def 餘閒相似度找文章(input_words):\n",
    "    articles_matrix_list=[]\n",
    "    for b in range(len(group_articles)):\n",
    "        結果=cos_sim(input_vector_matrix,articles_matrix[b])\n",
    "        articles_matrix_list.append(結果)\n",
    "#     print(\"第\",articles_matrix_list.index(max(articles_matrix_list)),\"篇新聞最相似\")\n",
    "    print(\"第\",articles_matrix_list.index(max(articles_matrix_list)),\"篇新聞最相似\")\n",
    "    哭喔 = articles_matrix_list.index(max(articles_matrix_list))\n",
    "    print(\"文章內文為:\",\"\\n\",\"\\n\",\"------------------------------------------------------------\",\"\\n\",np.array(group_articles[哭喔:哭喔+1]['content'])[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 斷詞套件\n",
    "def 斷詞(news):\n",
    "    df1=news\n",
    "    df1['seg']=None\n",
    "    df1['word_freq']=None\n",
    "    n=len(news)\n",
    "    alltext=[]\n",
    "    stopset=set()\n",
    "    stop2=['/n','']\n",
    "    with open('C:\\\\Users\\\\Big data\\\\Desktop\\\\字詞貼標\\\\DB104-master\\\\stop.txt','r',encoding='ISO-8859-1') as s:\n",
    "        for line in s:\n",
    "            stopset.add(line.strip('\\n'))\n",
    "    jieba.load_userdict('C:\\\\Users\\\\Big data\\\\Desktop\\\\字詞貼標\\\\DB104-master\\\\userdict.txt')\n",
    "    for i in range(n):\n",
    "        seg=''\n",
    "        wf={}\n",
    "        text= str(df1.loc[i]['content'])\n",
    "        cut=jieba.cut(text,cut_all=False)\n",
    "        for j in cut:\n",
    "            if j not in stopset:\n",
    "                seg +=j+' '\n",
    "                seg = seg.replace(\"\\n\",'')\n",
    "        df1['seg'][i]=seg\n",
    "        for w in seg.split(' '):\n",
    "            if w not in wf:\n",
    "                wf[w]=1\n",
    "            else:\n",
    "                wf[w]+=1\n",
    "        word_list=[(k,wf[k]) for k in wf if k not in stop2 ]\n",
    "        word_list.sort(key=lambda a :a[1],reverse=True)\n",
    "        df1['word_freq'][i]=word_list\n",
    "    return(df1)\n",
    "\n",
    "\n",
    "\n",
    "# 斷詞整理(去除標點符號)\n",
    "def 整理(df1):\n",
    "    datanews = df1[\"seg\"]\n",
    "    # 轉list\n",
    "    train_data = np.array(datanews)#np.ndarray()\n",
    "    datanews=train_data.tolist()#list\n",
    "    df1=''\n",
    "    for datanew in datanews:\n",
    "        df1+=str(datanew)+'\\n'\n",
    "    df1=df1.replace('[','').replace('\\'','').replace('「 ','').replace('」 ','').replace('」 ','').replace('、 ','').replace('： ','')\n",
    "    df1=df1.replace('《 ','').replace('》 ','').replace('） ','').replace('( ','').replace('／ ','').replace('， ','')\n",
    "    df1=df1.replace('╱ ','').replace('！ ','').replace('？ ','').replace('（ ','').replace('。 ','').replace('； ','').replace('… ','')\n",
    "    df1=df1.replace(']','').replace(' :','').replace('\\u3000','').replace('\\n','')\n",
    "    df2=df1\n",
    "    for i in range(len(df2)):\n",
    "        df1=' '.join(df2.split())\n",
    "    return(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
